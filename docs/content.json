{"pages":[],"posts":[{"title":"Week 1. x to X","text":"這門課在學什麼： 基礎機率，為何要學，它是什麼？要如何理解與應用它？（ Why? What? How? ） 我們生活在一個充滿不確定，隨機與大大小小誤差的世界 機率(與統計)是處理這些不確定/隨機/誤差的思考架構與處理想法/手法/程序的一個知識體 當我們具備有機率/統計一定程度的了解後，我們可以在這些紊亂中更有效的扒梳一些頭緒，犯較少錯誤；更進一步地，做出比較合理的判斷與預測 From x to X 簡單說，這門課可以視為我們學習由 x 到 X 的開始。 （小寫 x, 大寫 X) 回顧：數，未知數，隨機變數 例一：87, x, X (分別對應); 如： 某人的成績，某人的（未知）成績，該班同學的成績 例二：170，x, X (分別對應); 如： 某人某次身高測量值，某人某次的（未知）測量值，某人身高測量值 X, 如某班同學某次期中考的成績，東華彭魚雁身高的測量值 n 個觀察值的 枝葉圖，長條圖 （stem-and-leaf plot, histogram） 極限（$ n \\rightarrow \\infty $）標準化後(使得面積為 1）長條圖 ~~ 某些函數來反應/代表這樣的整體狀況","link":"/i2p2018/2018/10/27/w01/"},{"title":"Intro to Probability","text":"OverviewDe Finetti, 寫了兩大巨冊機率論的機率學者, 一言總結他書的要點 Probability does not exist. 機率不存在 機率不存在，那他在寫什麼？我們這門課又在學些什麼呢？其實，他是以文字矛盾來吸引我們的注意。但不只如此，他以很反諷的方式來提示—機率是我們思想的建構，它不是像石頭、飛鳥一樣的具體實際的物件，而是我們腦中所構想的概念。更重要的每個人也可能不盡相同，而時有不一致或矛盾的發生。 因此，建立一個不矛盾的架構，或大致上沒有不一致的機率規範，是讓我們了解機率，避免錯誤的一個開始。 這課是機率的第一門課。簡單總結，就是 $$\\Large x \\Rightarrow X.$$ Class Info Syllabus Navigator: C. Andy Tsao Office: SE A411. Tel: 3520 Lectures: Tue. 1310-1500, Thr. 1610-1700 @ AE 211Office Hours: Mon. 15:10-16:00, Thr. 12:10-13:00. @ SE A411 or by appointment. (Mentor Hour: Mon, Tue 1200-1300.) TA: 劉雅涵 何尚謙 Tel: 3517. ＠A408. Office Hour; 雅涵: 星期二 16:00-18:00 尚謙: 星期四 17:10-19:10 除了11/01(四)因有事,改為10/31(三) 17:00-19:00 Prerequisites: Calculus Textbook: Dekking, Kraaikamp, Lopuhaä and Meester (2005). A Modern Introduction to Probabilit and Statistics: Understanding Why and How. Springer, London. Legal downloadable from NDHU 期中考：11/6 (二) 1310–1440.","link":"/i2p2018/2018/10/07/hello-world/"},{"title":"Week 2. Probability Space","text":"The probabilistic way to formulate the big X: Notions, ideas and definitions Sample space, $\\Omega$: the set of all possible outcomes $\\mathcal{F}$ P, the probability function Two-envelop problem or switching paradox: What’s wrong with the argument?","link":"/i2p2018/2018/10/27/w02/"},{"title":"Week 3. Conditional probability and independence","text":"Homework 1Sec2.7 Exercises (Page 21) : 2.1, 2.3, 2.5, 2.7, 2.9, 2.11; 2.19 (optional) Class outline Conditional probability Bayes Theorem Independence","link":"/i2p2018/2018/10/27/w03/"},{"title":"Week 4. Discrete Random Variable-I","text":"Class Outline From x to X: Quantization of randomness/uncertainty Discrete random variable: Probability mass function, distribution function Bernoulli random variable Binomial random variable Toy example, real-world example, math/theoretical formulation: definitions and concepts Homework 2Sec 3.6 (Page 37): 3.1, 3.4, 3.10, 3.11, 3.12, 3.14, 3.18 (to be discussed in class on 10/11)","link":"/i2p2018/2018/10/27/w04/"},{"title":"Week 5-6. Discrete Random Variable-II","text":"Class outline Discrete random variable: Geometric random variable Characterization of discrete r.v.: probability mass function, distribution function Profiles of r.v.: E(X), Expectation (or expected value), and Var(X), Variance Computing E(X) and Var(X) for Bernoulli and Binomial random variables Homework 3Textbook Sec 4.6 (Page 51): 4.1, 4.2, 4.3, 4.4, 4.7, 4.8, 4.9, 4.14. To be discussed in class 10/25","link":"/i2p2018/2018/10/27/w05a6/"},{"title":"Week 7. Continuous Random Variable-I","text":"Class OutlineCompare and Contrast with discrete random variable From Discrete Uniform DU(K) to Uniform, U(0,1) pmf vs pdf Relation between the distribution function F(x) and pdf f(x) of a continous random variable $$F′(x)=f(x).$$ pdf is not a probability but a funciton/tool for computing probability. Specifically,$$f(x)\\neq P(X=x)$$and$$P(a≤X≤b)=\\int_a^bf(x)\\ dx.$$ Uniform U(0,1) Exponential random variable $Exp(\\lambda)$ Normal random variable $N(\\mu, \\sigma^2).$ (if time permits) Homework 4Textbook Sec 58 (Page 68): 5.1–5.6. To be discussed in class 11/1.","link":"/i2p2018/2018/10/27/w07/"},{"title":"Midterm Exam 期中考","text":"基礎機率期中考 時間：11/6 (二). 1310-1440。 地點： A210: 學號 大於等於 410511322 同學 A316: 學號 小於 410511322 同學 範圍：上課及習題內容。 其他：No cheatsheet nor mobile phone. Prepare early and Good Luck! 期中考成績統計 Min. Q1 Median Mean Q3 Max. 0.00 25.00 40.00 41.91 58.00 101.00 n=90; Sophomore (2): 3+ = 37: 53; sd. = 20.87 The decimal point is 1 digit(s) to the right of the | 0| 00555550003355578 2| 0003355589000233333555555 4| 000002355780000355555578889 6| 0225588000000359 8| 005010| 1 顏色意義：警示 危險 詳細分析/圖表 Statistics2016: Explore, Analysis","link":"/i2p2018/2018/11/05/w08/"},{"title":"Week 7.5.  Midterm 11/6 (二)","text":"Syllabus Navigator: C. Andy Tsao Office: SE A411. Tel: 3520 Lectures: Tue. 1310-1500, Thr. 1610-1700 @ AE 211 Office Hours: Mon. 15:10-16:00, Thr. 12:10-13:00. @ SE A411 or by appointment. (Mentor Hour: Mon, Tue 1200-1300.) TA: 劉雅涵 何尚謙 Tel: 3517. ＠A408. Office Hour; 雅涵: 星期二 16:00-18:00 尚謙: 星期四 17:10-19:10 除了11/01(四)因有事,改為10/31(三) 17:00-19:00 Textbook: Dekking, Kraaikamp, Lopuhaä and Meester (2005). A Modern Introduction to Probabilit and Statistics: Understanding Why and How. Springer, London. Legal download from NDHU 期中考：11/6 (二) 1310–1440.","link":"/i2p2018/2018/10/27/w07mid/"},{"title":"Week 11. Continuous Random Variable-III","text":"Normal Random Variable \\( X \\sim N(\\mu, \\sigma^2) \\Rightarrow \\frac{X - \\mu}{\\sigma} = Z \\sim N(0,1) \\).這是一個非常OP的定理。因為它使得任何一般常態隨機變數的機率計算都可以轉化為標準常態隨機變數的機率計算。具體來說 For any 0&lt;a&lt;b,$$P(a&lt;X&lt;b)=\\Phi( \\frac{b-\\mu}{\\sigma})-\\Phi(\\frac{a-\\mu}{\\sigma})$$where \\( \\Phi(x)=P(Z \\leq x),\\) cdf of \\( Z \\sim N(0,1).\\) Properties of \\( \\Phi, \\phi\\) (respectively, cdf and pdf of standard normal Z) \\( X \\sim N(\\mu, \\sigma^2)\\) Verify that pdf of X is indeed a pdf \\( E(X)=\\mu,\\ Var(X)= \\sigma^2. \\) Moment-generating function of X$$M(t)=E(e^{tX}) = \\exp({\\mu t + \\frac{\\sigma^2 t^2}{2}}) $$ Moment-generating function$$M(t)=E(e^{tX}). $$ 小用：計算/生成moments: for k, a positive integer$$E(X^k) = M^{(k)}(0)= M^{(k)}(t) |_{t=0} $$ if exists and \\( M^{(k)}(t)\\) is the k-th derivative of M(t). 大用：Identify the random variable. If exists, moment-generating function uniquely determines a random variable. 簡單說，mgf 唯一決定一個隨機變數。也就是說，若有兩個隨機變數有相同的 mgf 則他們有相同的分配。","link":"/i2p2018/2018/11/23/w11/"},{"title":"Week 10. Continuous Random Variable-II","text":"Normal Random Variable X is a normal random variable with E(X)= $\\mu \\in \\mathcal{R}$ and Var(X)= $\\sigma^2 &gt;0$$$ X \\sim N(\\mu, \\sigma^2)$$ pdf f(x), cdf F(X) $$ X \\sim N(\\mu, \\sigma^2) \\Rightarrow \\frac{X-\\mu}{\\sigma} \\sim N(0,1).$$ A N(0,1) random variable, i.e. a normal random variable with mean zero and variance 1, is commonly called a standard normal and denoted by Z pdf and cdf of Z ~ N(0,1). Their definitions and properties pdf$$ \\phi(x) = \\frac{1}{\\sqrt{2 \\pi }} e^{- x^2/2} $$ cdf$$ \\Phi(x) = \\int_{-\\infty}^x \\phi(t) dt.$$ Properties Homework 5Textbook Sec 5.8: Quick Exercises: 5.6, 5.7; Exercises: 5.13, 5.14. To be discussed in class 11/22.","link":"/i2p2018/2018/11/15/w10/"},{"title":"Week 12.1. 轉化 From X to g(X)","text":"轉換，轉化這門課，乃至於在大學課程中所學的隨機變數如Bernoulli, Binomial, Discrete Uniform; Uniform, Normal 等，都是如樂高積木的基本建構元件/模型。真實世界中的不確定現象，隨機性往往需要更複雜的模型。而函數正是我們由這些簡單元件堆疊出更複雜模型的一個非常重要的方式。 If X is a random variable and g is Borel-measurablethen g(X) is again a random variable. 以這門課來說，我們所使用的函數都是你所熟悉的函數，如\\(g(x)=a x +b, \\ g(x)=x^3 +2x \\)他們延伸出的 Y=g(X) 也都是隨機變數。 What is g(X)?但如何知道他們是什麼隨機變數呢？（這個問題翻成白話就是：Y的cdf \\(F_Y(y)\\), pdf/pmf \\(f_Y(y)\\), mgf \\( m_Y(t) \\) 只要找出三者之一，就完全刻劃了這個隨機變數）一般常用法 找出 \\( F_Y(y)\\) 與 \\(F_X(x) \\) 之間的關聯 找出 \\( M_Y(t)\\) 與 \\( M_X(t)\\) 之間的關聯，當 g(x) = a x +b 時特別好用 找出 \\( f_Y(y)\\) 與 \\(f_X(x) \\) 之間的關聯。當 f 是 pmf 時比較方便，f 是 pdf 時需要透過 F 的微分完成。當然，有一些一般的公式可以幫助計算，數統時會教。###例： X ~ U(0,1). Find cdf and pdf of Y wherea. \\(Y=g(X)=X^3\\),b.\\(Y=g(X)=(X-0.5)^2\\). \\( X \\sim N(\\mu, \\sigma^2) \\). Find pdf and cdf of$$Z= \\frac{X-\\mu}{\\sigma}$$ \\( X \\sim N(\\mu, \\sigma^2) \\). Find pdf and cdf of$$Y= a X + b$$ where a, b are real numbers. Expectation and Variance \\( E(a X +b) = a E(X) +b \\) \\( Var(aX+b) = a^2 Var(X)\\)provided E(X) and Var(X) both exist.","link":"/i2p2018/2018/11/28/w12/"},{"title":"Week 14.1. Random Vector. 1. Two Random Variables","text":"目前為止，我們已經學會 random variable 的一些基本。符號上來看，我們常以\\(X, Y\\) 來表示 random variable。接下來我們要拓展這個概念為 random vector. 符號上我們常用 \\( \\mathbf{X, Y}\\) 來區分。詳細地寫$$ \\mathbf{X}=(X_1, \\cdots, X_n)$$表示一個 n-random vector 其中 \\( X_1, \\cdots, X_n \\) 各是random variable。方便討論，先考慮 n=2 的情況。 發想例題考慮一個銅板投擲兩次的兩種投法：此銅板投擲一次的結果以Beroulli(p) 為模型，pmf 為 \\(f \\). 兩次投擲的結果以\\( X_1, X_2\\)表示。 \\( X_1, X_2\\) 為此銅板的一個 random sample of size 2。即兩次投擲皆來自同一銅板，且兩次投擲互不關聯，各自獨立。 投擲銅板一次 （\\(X_1\\)）後，即將第二次結果直接設為第一次結果（\\(X_2 \\leftarrow X_1\\)）。 引導問題 如何分別刻劃這兩種不同的隨機狀況？ ( \\( \\leadsto \\) joint pmf of \\(X_1, X_2\\) for these two scenarios ) \\( X_1, X_2\\) 個別隨機狀況與整體情形？( joint pmf and marginal pmf’s and relation between them) joint pmf \\( \\rightarrow\\) marginal pmf, but knowing marginal pmf of \\( X_1, X_2\\) cannot determine the joint pmf 對照於之前單一隨機變數，隨機向量也有相似的刻劃 joint pmf/pdf, joint cdf 其中 joint pmf/pdf 皆須滿足pmf 或pdf 的基本性質。除此之外，由joint pmf/pdf還可推導出 marginal pmf/pdf 值得提醒地，他們本身也還是pmf/pdf。因此也會滿足pmf 或是 pdf 的基本性質。再做相關計算的時候，這是一個迅速驗算的點。另外，類同地，pmf 可以理解為機率；而 pdf 並沒有機率的解釋，只是用來計算機率。 練習題請參考上課筆記","link":"/i2p2018/2018/12/12/w14/"},{"title":"Week 12.2. Transformation, characterization and profiling","text":"Profiling a random variable E(X) 與 Var(X) 可以理解為一個隨機變數的快照：E(X) 大致描述X的中央趨勢 (central tendency)而 Var(X) 則反映 X 的分散程度。更進一步Chebyshev’s InequalityLet X be a random variable with mean \\( E(X)= \\mu, Var(X)=\\sigma^2 \\). Then for any c &gt;0,$$ P(|X-\\mu| \\geq c) \\leq \\frac{Var(X)}{c^2}.$$ref: Textbook: 13.2, page 183 Remark Chebyshev’s bound is universal, i.e. for any random variable with finite mean and variance. Therefore, it may not be sharp for some random variables. Besides, it does not work well when c is small. Example: Let X be \\( N(\\mu, \\sigma^2)\\) and take \\( c=\\sigma/2, \\sigma, 2 \\sigma \\). Compute$$ P(|X-\\mu| \\geq c) $$and compare with the Chebyshev’s bound. Probability Integral formulaLet X be a random variable with cdf F. Then \\( F(X) \\sim U(0,1). \\)Hint of proof. You may start by assuming F has a inverse funciton \\( F_{-1}\\) to get some ideas about what’s going on. However, PIF holds without this assumption. Homework 6Textbook Sec 7.6: 7.2, 7.4, 7.8, 7.9, 7.15, 7.17; Sec 13.6: 13.1. To be discussed in class 12/06.","link":"/i2p2018/2018/11/30/w12b/"},{"title":"Week D-3. Independence of random variables","text":"DefinitionIndependence\\( X_1, \\cdots, X_n\\) are indepdendentiff for all \\(x_1, \\cdots, x_n \\in R\\)$$ f_{X_1, \\cdots, X_n}(x_1, \\cdots, x_n)= \\Pi_{i=1}^n f_{X_i}(x_i).$$iff for all \\(x_1, \\cdots, x_n \\in R\\)$$ F_{X_1, \\cdots, X_n}(x_1, \\cdots, x_n)= \\Pi_{i=1}^n F_{X_i}(x_i).$$ iid (independently identically distributioned)We say \\( X_1, \\cdots, X_n \\) are iid if \\( X_1, \\cdots, X_n\\) are indepdendent They have the same distribution, ie, \\( F_{i}(x)=F(x), f_{X_i}(x)=f(x)\\) for all i and where \\( F_{i}(x), f_{X_i}(x) \\) is the (marginal) cdf and pdf/pmf of \\(X_i \\) respectively. ImplicationsIf \\( X_1, \\cdots, X_n\\) are indepdendent then Probability $$ P(X_1 \\in A_1, \\cdots, X_n \\in A_n)= \\Pi_{i=1}^n P(X_i \\in A_i).$$ For any functions \\( g_i, \\ i=1, \\cdots, n\\)$$E(\\Pi_{i=1}^n g_i(X_i))= \\Pi_{i=1}^n E( g_i(X_i)$$provided the expectations on the right-hand side exist. It is much easier to work out what \\( Y=g(X_1, \\cdots, X_n )\\) is (ie, find out what its cdf or pdf/pmf or mgf is) or at least some snapshots such as expectation, variance or moments of \\( Y\\). Illustrations Let \\( X_1, \\cdots, X_n \\sim_{iid} Bernoulli(p), p \\in (0,1)\\) then$$ \\sum_{i=1}^n X_i \\sim Bin(n, p).$$ Let \\( X_1, \\cdots, X_n \\sim_{iid} N(\\mu, \\sigma^2))\\) then$$ \\bar{X}=\\frac{1}{n}\\sum_{i=1}^n X_i \\sim N(\\mu, \\frac{\\sigma^2}{n}).$$ Even when the distribution is only known to expectation and variance, we can still get some “pictures” of it. In the sense,if \\( X_1, \\cdots, X_n \\sim_{iid}\\) with \\( \\mu=E(X_i), \\sigma^2=Var(X_i)\\) then$$ E(\\bar{X})= \\mu, Var(\\bar{X})=\\frac{\\sigma^2}{n}.$$Note that this is consistent with our Bernoulli and normal cases. HomeworkTextbook: Sec. 8.6: 8.1, 8.5, 8.6, 8.9; Sec 9.7: 9.1, 9.3, 9.5, 9.10, 9.11, 9.12. Sec 10.5: 10.2, 10.6, 10.8, 10.16. To be discussed in class 12/27.","link":"/i2p2018/2018/12/20/wm3/"}],"tags":[{"name":"transformation, charaterization, expectation, variance, mgf","slug":"transformation-charaterization-expectation-variance-mgf","link":"/i2p2018/tags/transformation-charaterization-expectation-variance-mgf/"},{"name":"random vector","slug":"random-vector","link":"/i2p2018/tags/random-vector/"},{"name":"transformation, characterization, mgf, Chebyshev's inequality, probability integral formula","slug":"transformation-characterization-mgf-Chebyshev-s-inequality-probability-integral-formula","link":"/i2p2018/tags/transformation-characterization-mgf-Chebyshev-s-inequality-probability-integral-formula/"},{"name":"Independence, iid, sum of random variables","slug":"Independence-iid-sum-of-random-variables","link":"/i2p2018/tags/Independence-iid-sum-of-random-variables/"}],"categories":[]}